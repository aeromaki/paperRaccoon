{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93218688dbcb47e1a72eb27358d67e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcea0618fe434fc08c43886cba4a8447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: pubmed-summarization/section\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset pubmed-summarization/section to /Users/persica/.cache/huggingface/datasets/ccdv___pubmed-summarization/section/1.0.0/f765ec606c790e8c5694b226814a13f1974ba4ea98280989edaffb152ded5e2b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f5e26af8ff4afe8193b8987b73e373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/779M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05ddff55c8f47b1af1b5995a72bf888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fd988260b04a3598c61d352344472c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089bbcc42b3644509d207839d4fa787d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d036b3d8f0c446eeabd240e565b6427c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91360305678b44ae845635fee22f7320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pubmed-summarization downloaded and prepared to /Users/persica/.cache/huggingface/datasets/ccdv___pubmed-summarization/section/1.0.0/f765ec606c790e8c5694b226814a13f1974ba4ea98280989edaffb152ded5e2b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b10e193bd784d569bc1b817ce0ff0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "dataset = load_dataset(\"ccdv/pubmed-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 119924\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6633\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6658\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 232/10000 [00:04<02:22, 68.70it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (24699 > 16384). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 10000/10000 [02:15<00:00, 73.55it/s]\n",
      "100%|██████████| 10000/10000 [02:15<00:00, 73.67it/s]\n",
      "100%|██████████| 10000/10000 [02:12<00:00, 75.48it/s]\n",
      "100%|██████████| 10000/10000 [02:15<00:00, 74.07it/s]\n",
      "100%|██████████| 10000/10000 [02:11<00:00, 75.92it/s]\n",
      "100%|██████████| 10000/10000 [02:13<00:00, 74.66it/s]\n",
      "100%|██████████| 10000/10000 [02:10<00:00, 76.85it/s]\n",
      "100%|██████████| 10000/10000 [02:16<00:00, 73.35it/s]\n",
      "100%|██████████| 10000/10000 [02:14<00:00, 74.55it/s]\n",
      "100%|██████████| 10000/10000 [02:13<00:00, 74.92it/s]\n",
      "100%|██████████| 10000/10000 [02:12<00:00, 75.34it/s]\n",
      "100%|██████████| 9924/9924 [02:13<00:00, 74.11it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 984.50it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 995.08it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 986.62it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 986.91it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 984.11it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 966.93it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 958.78it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 966.58it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 986.18it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 971.94it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 971.68it/s]\n",
      "100%|██████████| 9924/9924 [00:10<00:00, 982.24it/s] \n",
      "100%|██████████| 6658/6658 [01:32<00:00, 72.30it/s]\n",
      "100%|██████████| 6658/6658 [00:06<00:00, 1001.32it/s]\n",
      "100%|██████████| 6633/6633 [01:32<00:00, 71.60it/s]\n",
      "100%|██████████| 6633/6633 [00:06<00:00, 992.49it/s] \n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "num_rows = dataset[\"train\"].num_rows\n",
    "end_rg = num_rows % batch_size\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"train\"][\"article\"][i:i+rg])]\n",
    "    with open(file=f\"./pubmed_train_article/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"train\"][\"abstract\"][i:i+rg])]\n",
    "    with open(file=f\"./pubmed_train_abstract/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf\n",
    "\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "num_rows = dataset[\"test\"].num_rows\n",
    "end_rg = num_rows % batch_size\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"test\"][\"article\"][i:i+rg])]\n",
    "    with open(file=f\"./pubmed_test_article/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"test\"][\"abstract\"][i:i+rg])]\n",
    "    with open(file=f\"./pubmed_test_abstract/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf\n",
    "\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "num_rows = dataset[\"validation\"].num_rows\n",
    "end_rg = num_rows % batch_size\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"validation\"][\"article\"][i:i+rg])]\n",
    "    with open(file=f\"./pubmed_validation_article/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"validation\"][\"abstract\"][i:i+rg])]\n",
    "    with open(file=f\"./pubmed_validation_abstract/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg",
   "language": "python",
   "name": "hg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
