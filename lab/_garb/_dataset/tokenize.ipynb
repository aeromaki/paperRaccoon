{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ccdv/arxiv-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 203037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract'],\n",
       "        num_rows: 6440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/10000 [00:01<19:18,  8.62it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (21321 > 16384). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 10000/10000 [04:46<00:00, 34.90it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "num_rows = dataset[\"train\"].num_rows\n",
    "end_rg = num_rows % batch_size\n",
    "\n",
    "for i in range(0, 1): #num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"train\"][\"article\"][i:i+rg])]\n",
    "    with open(file=f\"./tokenized_train_article/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 87/10000 [00:01<01:57, 84.06it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (20889 > 16384). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 736.74it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 830.79it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 845.93it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 848.94it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 826.93it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 790.64it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 804.32it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 848.38it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 819.90it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 835.48it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 868.40it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 822.40it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 842.83it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 837.17it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 865.29it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 826.56it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 835.61it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 797.62it/s]\n",
      "100%|██████████| 10000/10000 [00:11<00:00, 866.29it/s]\n",
      "100%|██████████| 10000/10000 [00:15<00:00, 654.99it/s]\n",
      "100%|██████████| 3037/3037 [00:06<00:00, 494.13it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "num_rows = dataset[\"train\"].num_rows\n",
    "end_rg = num_rows % batch_size\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"train\"][\"abstract\"][i:i+rg])]\n",
    "    with open(file=f\"./tokenized_train_abstract/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6440/6440 [02:47<00:00, 38.38it/s]\n",
      "100%|██████████| 6440/6440 [00:05<00:00, 1257.85it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "num_rows = dataset[\"test\"].num_rows\n",
    "end_rg = num_rows % batch_size\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"test\"][\"article\"][i:i+rg])]\n",
    "    with open(file=f\"./tokenized_test_article/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"test\"][\"abstract\"][i:i+rg])]\n",
    "    with open(file=f\"./tokenized_test_abstract/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6436/6436 [02:47<00:00, 38.39it/s]\n",
      "100%|██████████| 6436/6436 [00:04<00:00, 1318.33it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "\n",
    "num_rows = dataset[\"validation\"].num_rows\n",
    "end_rg = num_rows % batch_size\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"validation\"][\"article\"][i:i+rg])]\n",
    "    with open(file=f\"./tokenized_validation_article/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf\n",
    "\n",
    "for i in range(0, num_rows, batch_size):\n",
    "    rg = end_rg if i == num_rows - end_rg else batch_size\n",
    "    buf = [tokenizer(seq, return_tensors=\"pt\").input_ids for seq in tqdm(dataset[\"validation\"][\"abstract\"][i:i+rg])]\n",
    "    with open(file=f\"./tokenized_validation_abstract/{i}.pickle\", mode='wb') as f:\n",
    "        pickle.dump(buf, f)\n",
    "    del buf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg",
   "language": "python",
   "name": "hg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
