{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02959f1a-442f-4318-be2c-9e028d9d8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541fd476-0176-4c75-a9c5-70c6e672ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8b10b1d016497ba5a1028d5d44b37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef33937-41f9-47e0-9d6e-87b73221793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem(x):\n",
    "    s = x.split(\"--\")\n",
    "    if len(s) < 2:\n",
    "        return x\n",
    "    else:\n",
    "        return \"--\".join(s[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42056c60-2282-4096-a4ba-42c9625289e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_num(max_num):\n",
    "    return torch.randint(max_num,(1,))[0]\n",
    "\n",
    "def random_place(max_idx, num):\n",
    "    rand = torch.randperm(max_idx)[:num].sort().values\n",
    "    return rand[5:] if len(rand) > 8 else rand\n",
    "\n",
    "def random_token_number():\n",
    "    #return tokenizer.encode(f\"{random_num(10000)} \")[1:-1]\n",
    "    return tokenizer.encode(f\"{random_num(10000)/(3+random_num(100)):.2} \")[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fdfa824-1482-41fe-81c5-fe05f96137cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bos_eos(list_seq):\n",
    "    return torch.cat([torch.tensor([tokenizer.bos_token_id]), torch.tensor(list_seq), torch.tensor([tokenizer.eos_token_id])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cbc2387-baeb-48b5-88bc-95a02bf6bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(input_text, max_seq, rto):\n",
    "    enc = tokenizer.encode(input_text)[1:-1]\n",
    "    ll = min(round(max_seq * (1-rto)), len(enc))\n",
    "    enc = enc[:ll]\n",
    "    rd = random_place(ll, random_num(min(max_seq, round(ll / (1-rto))) - ll))\n",
    "\n",
    "    collect = []\n",
    "    onehot = []\n",
    "\n",
    "    idx = 0\n",
    "    l = len(rd)\n",
    "    for i in range(ll):\n",
    "        if i < l and i == rd[idx]:\n",
    "            for j in range(random_num(20)):\n",
    "                rand = random_token_number()\n",
    "                collect += rand\n",
    "                onehot += [1 for _ in rand]\n",
    "            idx += 1\n",
    "        \n",
    "        collect += [enc[i]]\n",
    "        onehot += [0]\n",
    "\n",
    "    input_ids = torch.ones(max_seq, dtype=torch.int64) * tokenizer.pad_token_id\n",
    "    e_col = bos_eos(collect[:max_seq-2])\n",
    "    input_ids[:len(e_col)] = e_col\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    onehot = torch.tensor([0] + onehot[:max_seq-2] + [0], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    label = torch.zeros((onehot.shape[0], max_seq))\n",
    "    label[:, :onehot.shape[1]] = onehot\n",
    "\n",
    "    return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28e7201-0970-41e4-bdee-88eaf45f5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "class RoBERTa_Denoiser(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.roberta_encoder = AutoModel.from_pretrained(\"xlm-roberta-large\")\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden_state = self.roberta_encoder(input_ids, attention_mask).last_hidden_state\n",
    "\n",
    "        output = torch.zeros((hidden_state.shape[0], 512, 1024)).to(self.device)\n",
    "        output[:, :hidden_state.shape[1], :] = hidden_state\n",
    "\n",
    "        output = self.head(output).squeeze(-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806e4035-8fde-4887-8db7-708b03fe318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "denoiser = RoBERTa_Denoiser(device).to(device)\n",
    "denoiser.load_state_dict(torch.load(\"denoiser_roberta_rto_10000.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b20e57a2-f395-4f95-8154-08dce42707ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(pred, labels, weight):\n",
    "    sigpred = pred.sigmoid()\n",
    "    return (-(labels * sigpred.log() + (1-labels) * (1-sigpred).log()) * (labels * (weight-1) + 1)).sum()\n",
    "\n",
    "optimizer = optim.Adagrad(denoiser.parameters(), lr=3e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f488e9-6e67-40f6-9e02-8df8fbd7a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataload(i, n, max_seq, noise_rto):\n",
    "    b_input_ids = []\n",
    "    b_labels = []\n",
    "    for d in cnn[\"train\"][\"article\"][i:i+n]:\n",
    "        input_ids, label = noise(rem(d), max_seq, noise_rto)\n",
    "        b_input_ids += [input_ids]\n",
    "        b_labels += [label]\n",
    "\n",
    "    return torch.cat(b_input_ids, dim=0), torch.cat(b_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c172389-3cc4-427f-ad54-af774f008c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "max_seq_len = 512\n",
    "print_size = 50\n",
    "weight = 25\n",
    "noise_rto = 0.2\n",
    "\n",
    "start_point = 12000\n",
    "num = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6d4d199-37ac-493f-ae18-1c6685e7cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626.77640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/250 [01:14<05:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301.2696774291992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 100/250 [02:30<03:44,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.21316438674927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 150/250 [03:45<02:32,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.1456770324707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 200/250 [05:00<01:15,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.73045713424682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [06:15<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "buf = 0\n",
    "denoiser.train()\n",
    "for i in tqdm(range(start_point, start_point+num, batch_size)):\n",
    "    input_ids, labels = dataload(i, batch_size, max_seq_len, noise_rto)\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    pred = denoiser(input_ids)\n",
    "    loss = criterion(pred, labels, weight)\n",
    "    buf += loss.item()\n",
    "    if i % (batch_size * print_size) == 0:\n",
    "        print(buf / print_size)\n",
    "        buf = 0\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c0ae5b1-0829-4618-aa7b-194b710e9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(denoiser.state_dict(), \"./denoiser_roberta_rto_num_14000.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd22826d-4e03-459d-9122-0b1054d17699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bos_eos(list_seq):\n",
    "    return torch.cat([torch.tensor([tokenizer.bos_token_id]), torch.tensor(list_seq), torch.tensor([tokenizer.eos_token_id])], dim=0)\n",
    "\n",
    "def denoise(text, max_seq_len=512):\n",
    "    enc = tokenizer.encode(text)[1:-1]\n",
    "    ll = len(enc)\n",
    "    chunk = max_seq_len - 2\n",
    "\n",
    "    ret = []\n",
    "    garb = []\n",
    "    for i in range(0, ll, chunk):\n",
    "        input_ids = bos_eos(enc[i:i+chunk]).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            remove = (denoiser(input_ids) > 0).to(\"cpu\")\n",
    "        \n",
    "        for j, k in zip(input_ids[0,1:-1], remove[0,1:-1]):\n",
    "            if not k:\n",
    "                ret += [j]\n",
    "            else:\n",
    "                garb += [tokenizer.decode([j])]\n",
    "\n",
    "    return tokenizer.decode(ret), garb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b1239d0-8708-44dd-8bb5-f6fa6024412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"   'text': 'Some input sequences still exceed LoBART’s longer fixed-span limit. Further extending the input span would lead to a small local attention span, a diminishing improvement, or GPU running out of memory. Alternatively, it has been shown that a better content selection improves abstractive summarization in news ( Chen and Bansal ,  2018 ; Gehrmann et al. ,  2018 ;  Hsu et al. ,  2018 ), multi doc- uments ( Liu and Lapata ,  2019a ;  Liu et al. ,  2018 ), and scientific articles ( Pilault et al. ,  2020 ). Thus, we propose to tackle the excess length by content selection. Here, we distinguish between two phases of content selection: training time and test time. 5.1 Training-time Content Selection During training, ground-truth targets are available. We categorize selection methods in this phase into two types: ground-truth based (model-free), which is also referred to as  oracle ; and model-based. Ground-truth based methods cannot be used at test time, while model-based methods can be applied at both phases. Although model-based methods do not rely on ground-truth targets, they have the advantage of matching in training and test phases. Existing oracle methods include using ROUGE-2 recall ( Liu et al. ,  2018 ) or the average of ROUGE-1,2,L recall ( Pilault et al. ,  2020 ). We discuss model-based methods in Section  5.2 , where we propose the MCS method. Let the subscript  ( i, j )  denote the position of the  j -th word in the  i -th input sentence, the full input  X = { x 1 , ...,  x i , ...,  x N 1 } = � [ x 1 , 1 , x 1 , 2 , x 1 ,J 1 �� � sent  1 , ..., x i, 1 , x i,J i � �� � sent  i , ..., x N 1 , 1 , x N 1 ,J N 1 � �� � sent  N 1 ] . Content selection re-ranks, truncates, and sorts  X to get  X cs   for training BART/LoBART as follows:  ̄ X  =  { x r 1 ,  x r 2 ,  x r 3 , ...,  x r R } (2) X cs   =  SortOrig ( TruncateN (   ̄ X )) (3) where  r i  is the index of the sentence of rank  i , the TruncateN  operation filters    ̄ X  such that the total of number of words is less than  N , and  SortOrig retains the original sentence order. The following ranking methods are considered: • Truncation (TRC):  r k  =  k . • Model-based: Given the score  f  of model  φ , r k  =  { i  ∈  N 1  :  f φ ( i | X )  is ranked  k -th } •  Oracle (ORC): Given the ground-truth sum- mary  y  and similarity measure  d , r k  =  { i  ∈  N 1  :  d ( x i ,  y )  is ranked  k -th } In this work, we use ROUGE-2 recall as the sim- ilarity measure  d . For the ORC method, first, we retain only sentences with positive  d , leading to R  ≤  N 1 . We found that the number of sentences with positive  d  is low at 21.3% of the total number of sentences in average on podcast data. This cor- responds to 56% of training instances being shorter than BART input span of 1024. 6   This no-padding oracle method (ORC no-pad ) is highly  aggressive , potentially preventing the downstream summarizer 6 We refer to this percentage as %AgORC no-pad  (the per- centage of inputs aggressively extracted by the oracle method). from learning complex abstraction. Hence, we propose variants of oracle methods to extend the ORC no-pad -selected input to the max input span  N : •  ORC pad-lead : Pad by leading unselected sen- tences and keep the original sentence order. •  ORC pad-rand : Pad by random unselected sen- tences and keep the original sentence order. TRC MCS ORC-pad-lead ORC-pad-rand ORC-no-pad 22.0 24.0 26.0 28.0 30.0 32.0 34.0 ROUGE-1 (F1) 27.88 28.14 29.99 30.39 32.39 26.82 27.24 26.34 27.28 25.26 26.43 26.32 24.78 25.54 22.71 Abstractive Generation Performance of Downsteam BART TestTime: Oracle (UpperBound) TestTime: MCS (CurrentBest) TestTime: Truncate (Baseline) In Figure  5 , since any oracle method is consid- ered cheating at test time, the best performance is obtained by MCS (in blue), and the upper bound performance is obtained by optimal oracle method (in green). The results show that although ORC no-pad  yields the highest upper bound, the ab- stractive model in fact does not learn how to per- form abstraction. For instance, with TRC or MCS at test time, ORC no-pad  yields the lowest perfor- mance level. The best way to fine-tune the abstrac- tive model shown in Figure  5  is using ORC pad-rand . Compared to ORC pad-lead , ORC pad-rand  is better as it introduces more diversity to the abstractive model. Compared to the model-based method, ORC pad-rand is also computationally less expensive. In addition, Table  5  shows that when there is no content selection at test time (i.e. TRC ap- plied), LoBART(4k) and LoBART(8k) benefit from ORC pad-rand , whereas BART(1k) does not. This is because in the 1k setting, content selection is more aggressive; as a result, the large mismatch between training and test leads to a poor result. Thus, we suggest that the best content selection during train- ing is ORC pad-rand  given that content selection will be used at test time, or model’s input span is long. 5.2 Multitask Content Selection (MCS) To process long input sequences entirely, we con- sider RNN, whose memory requirement grows lin- early with the sequence length, and hierarchical architectures which have been shown effective for long seq2seq tasks ( Cohan et al. ,  2018 ;  Li et al. , 2019 ). In this work, the hierarchical RNN model described in Section  3.2  has memory requirement given the target length of 144 during training of 0 . 83+ B (3 . 96 × 10 − 5 +3 . 33 × 10 − 5 N 2 ) N 1 , 7   where N 1  is #sentences, and  N 2  is the maximum number of words in a sentence, and  B  is batch size. By setting  N 1 =1000 and  N 2 =50, only 2% of podcast data exceeds this limit, while taking GPU memory to only 2.53GiB for  B =1. Thus, this shows that this model can cover long sequences. Previous model-based methods treat content se- lection as extractive labelling and create labels heuristically ( Pilault et al. ,  2020 ), or using encoder- decoder attention mechanism ( Manakul and Gales , 2020 ). To utilize both of these in one framework, we propose a Multitask Content Selection (MCS) method where we train the hierarchical encoder- decoder with attention mechanism and a classifi- cation layer on top of the encoder (described in Section  3.2 ). First, the model is trained on seq2seq abstractive summarization objective: L seq2seq  =  − M � m =1 log  P ( y m | y <m ,  X ) (4) Second, we create binary labels as follows: for sentence  i , the label  z i  is 1 if  d ( x i ,  y )  >  0 ; else  z i is 0, and  d  is the ROUGE-2 recall measure. The extractive labelling task objective is: L label  =  −   � N 1 i =1   ( z i  log ˆ z i  + (1  −  z i ) log(1  −  ˆ z i ))  (5) ˆ z i  =  sigmoid ( W T cls h i   +  b cls ) (6) where  h i  is the sentence-level encoder output as- sociated with sentence  i , and  W cls ,  b cls  are the parameters of the classification layer. Thus, the MCS training loss is defined as follows: L MCS  =  γ L label  + (1  −  γ ) L seq2seq (7) At inference stage, there are two modes: (i) stan- dard abstractive summary generation, e.g. via beam search decoding; (ii) ranking input sentences via labelling score and seq2seq attention score. The latter is how we use MCS during inference. 8   For sentence  i , the scores are: score i, ( label )  = ˆ z i ,  score i, ( seq2seq )  =   � M m =1   α s m,i (8) 7 Obtained by least-squares regression with 20 samples. 8 In practice, we run beam search decoding of width 4, and we obtain the attention score from the top beam. where  α s m,i   is the sentence-level attention weight at decoder step  m  over input sentence  i . Since the scores are on different scales, rather than using the scores defined in Eq.  8 , we simply rank the scores, and then normalize the score ranks into the range 0.0 to 1.0. Let nscore denote the normalized ranking score, the MCS inference score is: f φ ( i | X ) =  nscore i, ( label )  +  nscore i, ( seq2seq ) (9) In our preliminary experiments, we vary the amount of selected sentences from the limit of BART/LoBART to a few sentences, and we found that more aggressive selection at test time degrades the performance. Therefore, our MCS selects input sentences up to the limit of BART/LoBART. By setting  γ =0.0, our method is comparable to the attention-based method in  Manakul and Gales ( 2020 ). By setting  γ =1.0, our method is similar to the extractive models in  Hsu et al.  ( 2018 );  Pi- lault et al.  ( 2020 ). In Table  4 , we show that when coupled with BART, MCS yields better summariza- tion performance than both Attn-only and Ext-only baselines. MCS also achieves higher recall rate of sentences with  d ( x i ,  y )  >  0  than the two baselines. System %Recall R1 R2 RL Attn ( L seq2seq ) 38.85 26.90 9.70 18.78 Ext ( L label ) 35.26 26.39 8.90 18.03 MCS ( L MCS ) 40.50 27.28 9.82 19.00 '},\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e273316-ff09-4aba-a6c6-0dad90400588",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, garb = denoise(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9ef335f-fbd8-4997-8eac-7651dd6382cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"'text': 'Some input sequences still exceed LoBART’s longer fixed-span limit. Further extending the input span would lead to a small local attention span, a diminishing improvement, or running out of memory. Alternatively, it has been shown that a better content selection improves abstractive summarization in news ( Chen and Ban, 2018 ; Gehrmann et al., 2018 ; Hsu et al., 2018 ), multi docments ( Liu and La, ; Liu et al., 2018), and scientific articles ( Pilault et al., 2020). Thus, we propose to tackle the excess length by content selection. Here, we distinguish between two phases of content selection: training time and test time. 5.1 Training-time Content Selection During training, ground-truth targets are available. We categorize selection methods in this phase into two types: ground-truth based (model-free), which is also referred to as oracle ; and model-based. Ground-truth based methods cannot be used at test time, while model-based methods can be applied at both phases. Although model-based methods do not rely on ground-truth targets, they have the advantage of matching in training and test phases. Existing oracle methods include using ROUGE-2 recall ( Liu et al., 2018 ) or the average of ROUGE- recall ( Pilault et al., 2020). We discuss model-based methods in Section 5.2, where we propose the MCS method. Let the subscript ( i, ) denote the position of theth word in the ith input sentence, the full input X = {, } = [,,,, ]. Content selection re-ranks, truncates, and sorts X to get X for training BART/LoBART as follows: X = { x 1,,,, } X =Origncate where r is the index of the sentence of, the Truncate operation filters X such that the total of number of words is less than N, and SortOrig retains the original sentence order. The following ranking methods are considered: • Truncation (TRC): r k = k. • Model-based: Given the score of model, r k = { i∈ ( i X ) is ranked kth • Oracle (ORC): Given the ground-truth sum-y and similarity measure, r k = { i∈, y ) is ranked kth } In this work, we use ROUGE as the sim- ilarity measure d. For the ORC method, first, we retain only sentences with positive d, leading to R ≤ N 1. We found that the number of sentences with positive d is low at 21.3% of the total number of sentences in average on podcast data. This- responds to56% of training instances being shorter than B input span of 1024. 6 This no-padding oracle method (ORC no-pad ) is highly aggressive, potentially preventing the downstream summarizer 6 We refer to this percentage as %ORC no-pad (the per-age of inputs aggressively extracted by the oracle method). from learning complex abstraction. Hence, we propose variants of oracle methods to extend the ORC no-pad -selected input to the max input span : • ORC pad-lead : Pad by leading unselected sen- tences and keep the original sentence order. • ORC pad-rand : by random unselected sen- tences and keep the original sentence order. ORC-pad-lead ORC-pad-rand ORC-no-padOUGE ( Abstractive Performance ofsteART Test: (Upperund) Test: MCS (Current) Test: Trun (Baseline) In Figure 5, since any oracle method is consid- ered cheating at test time, the best performance is obtained by MCS (in blue), and the upperbound performance is obtained by optimal oracle method (in green). The results show that although ORC no-pad yields the highest upperbound, the ab- stractive model in fact does not learn how to per- form abstraction. For instance, with TRC or MCS at test time, ORC no-pad yields the lowest perfor- mance level. The best way to fine-tune the abstrac-tive model shown in Figure 5 is using ORC pad-rand. Compared to ORC pad-le, ORC pad-rand is better as it introduces more diversity to the abstractive model. Compared to the model-based method, ORC pad-rand is also computationally less expensive. In addition, Table 5 shows that when there is no content selection at test time (i.e. TRC ap-ed), LoBART(4k) and LoBART(8k) benefit from ORC pad-rand, whereas BART(1k) does not. This is because in the 1k setting, content selection is more aggressive; as a result, the large mismatch between training and test leads to a poor result. Thus, we suggest that the best content selection during train- ing is ORC pad-rand given that content selection will be used at test time, or model’s input span is long. 5.2 Multitask Content Selection (MCS) To process long input sequences entirely, we con- RNN, whose memory requirement grows lin with the sequence length, and hierarchicalarchitectures which have been shown effective for long seqse tasks ( Cohan et al., 2018 ; Li et al., 2019). In this work, the hierarchical RNN model described in Section 3.2 has memory requirement given the target length of during training of 0.. 1, where N 1 issent, and N 2 is the maximum number of words in a sentence, and B is batch size. By setting N 1 = and N 2 =, only 2% of data exceeds this limit, while taking memory to only 2.5B for B. Thus, this shows that this model can cover long sequences. Previous model-based methods treat content se- lection as extractive labelling and create labels heuristically ( Pilault et al., 2020 ), or using encoder- decoder attention mechanism ( Manakul and Gales, 2020 ). To utilize both of these in one framework, we propose a Multitask Content Selection (MCS) method where we train the hierarchical encoder- decoder with attention mechanism and a classifi-cation layer on top of the encoder (described in Section 3.2 ). First, the model is trained on seqq abstractive summar objective: seqseq = ( y, ) Second, we create binary labels as follows: for sentence i, the label z i is 1 if d ( x i, y ) > 0 ; else z i is 0, and d is the ROUGE recall measure. The extractive labelling task objective is: L label = − i=1 z i + z i z i )) (5)ˆ z i = sigmoid ( cl + b cls ) (6) where h i is the sentence-level encoder output asted with sentence i, and cls, b cls are the parameters of the classification layer. Thus, the MCS training is defined as follows: L MCS = L + ) L seqseq At inference stage, there are two modes: (i) stan- dard abstractive summary generation, e.g. via beam search decoding; (ii) ranking input sentences via label score and seqq attention score. The latter is how we use MCS during inference. For sentence, the scores are: score i ( =, score i (qq ) =i Obtained by least-square regression with 20 samples. In practice, we run beam decoding ofwidth, and we obtain the attention score from the top beam. where si is the sentence-level attention weight at deco input sentence i. Since the scores are on different scales, rather than using the scores defined in Eq. 8, we simply rank the scores, and then normalize the score ranks into the range 0.0 to 1.0. Letnscore denote the normalized ranking score, the MCS inference score is: f ( i ) =core i, ( ) +core i, ( seqq ) In our preliminary experiments, we vary the amount of selected sentences from the limit of BART/LoBART to a few sentences, and we found that more aggressive selection at test time degrades the performance. Therefore, our MCS selects input sentences up to the limit of BART/LoBART. By setting γ =0.0, our method is comparable to the attention-based method in Manakul and Gales ( 2020). By setting γ =1.0, our method is similar to the extractive models in Hsu et al. ( 2018); Pi-ult et al. ( 2020). In Table 4, we show that when coupled with BART, MCS yields better summa- tion performance than both Attn-only and Ext-only baselines. MCS also achieves higher recall rate of sentences with d ( x i, y ) > 0 than the two baselines. (qq (CS (CS},\",\n",
       " ['GPU',\n",
       "  'sal',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '-',\n",
       "  'u',\n",
       "  'pata',\n",
       "  '',\n",
       "  '2019',\n",
       "  'a',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '1,2',\n",
       "  ',',\n",
       "  'L',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'j',\n",
       "  'j',\n",
       "  '-',\n",
       "  '-',\n",
       "  'x',\n",
       "  '1',\n",
       "  '',\n",
       "  ',',\n",
       "  '...',\n",
       "  ',',\n",
       "  'x',\n",
       "  'i',\n",
       "  '',\n",
       "  '...',\n",
       "  ',',\n",
       "  'x',\n",
       "  'N',\n",
       "  '1',\n",
       "  'x',\n",
       "  '1',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  ',',\n",
       "  'x',\n",
       "  '1',\n",
       "  '',\n",
       "  ',',\n",
       "  '2',\n",
       "  '',\n",
       "  ',',\n",
       "  'x',\n",
       "  '1',\n",
       "  '',\n",
       "  ',',\n",
       "  'J',\n",
       "  '1',\n",
       "  'sent',\n",
       "  '1',\n",
       "  '',\n",
       "  ',',\n",
       "  '...',\n",
       "  'x',\n",
       "  'i',\n",
       "  ',',\n",
       "  '1',\n",
       "  '',\n",
       "  ',',\n",
       "  'x',\n",
       "  'i',\n",
       "  ',',\n",
       "  'J',\n",
       "  'i',\n",
       "  'sent',\n",
       "  'i',\n",
       "  '',\n",
       "  ',',\n",
       "  '...',\n",
       "  ',',\n",
       "  'x',\n",
       "  'N',\n",
       "  '1',\n",
       "  '',\n",
       "  ',',\n",
       "  '1',\n",
       "  '',\n",
       "  'x',\n",
       "  'N',\n",
       "  '1',\n",
       "  '',\n",
       "  'J',\n",
       "  'N',\n",
       "  '1',\n",
       "  'sent',\n",
       "  'N',\n",
       "  '1',\n",
       "  '',\n",
       "  'cs',\n",
       "  '',\n",
       "  '̄',\n",
       "  'r',\n",
       "  '',\n",
       "  'x',\n",
       "  'r',\n",
       "  '2',\n",
       "  '',\n",
       "  'x',\n",
       "  'r',\n",
       "  '3',\n",
       "  '',\n",
       "  '...',\n",
       "  'x',\n",
       "  'r',\n",
       "  'R',\n",
       "  '(2)',\n",
       "  'cs',\n",
       "  'Sort',\n",
       "  '(',\n",
       "  'Tru',\n",
       "  'N',\n",
       "  '(',\n",
       "  '',\n",
       "  '̄',\n",
       "  'X',\n",
       "  '))',\n",
       "  '(3)',\n",
       "  'i',\n",
       "  'rank',\n",
       "  'i',\n",
       "  '',\n",
       "  'N',\n",
       "  '',\n",
       "  '̄',\n",
       "  '',\n",
       "  '',\n",
       "  'f',\n",
       "  'φ',\n",
       "  '',\n",
       "  '',\n",
       "  'N',\n",
       "  '1',\n",
       "  ':',\n",
       "  'f',\n",
       "  'φ',\n",
       "  '',\n",
       "  '|',\n",
       "  '-',\n",
       "  '}',\n",
       "  'mar',\n",
       "  'y',\n",
       "  'd',\n",
       "  '',\n",
       "  '',\n",
       "  'N',\n",
       "  '1',\n",
       "  ':',\n",
       "  'd',\n",
       "  '(',\n",
       "  'x',\n",
       "  'i',\n",
       "  '',\n",
       "  '-',\n",
       "  '-2',\n",
       "  'recall',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'cor',\n",
       "  '',\n",
       "  'ART',\n",
       "  '',\n",
       "  'Ag',\n",
       "  'cent',\n",
       "  'N',\n",
       "  'Pad',\n",
       "  'TR',\n",
       "  'C',\n",
       "  'M',\n",
       "  'CS',\n",
       "  '2',\n",
       "  '2.0',\n",
       "  '2',\n",
       "  '4.0',\n",
       "  '26.',\n",
       "  '0',\n",
       "  '28.',\n",
       "  '0',\n",
       "  '3',\n",
       "  '0.0',\n",
       "  '3',\n",
       "  '2.0',\n",
       "  '3',\n",
       "  '4.0',\n",
       "  'R',\n",
       "  '-1',\n",
       "  'F',\n",
       "  '1)',\n",
       "  '27.',\n",
       "  '88',\n",
       "  '28.',\n",
       "  '14',\n",
       "  '2',\n",
       "  '9.99',\n",
       "  '30',\n",
       "  '.',\n",
       "  '39',\n",
       "  '32.',\n",
       "  '39',\n",
       "  '26.',\n",
       "  '82',\n",
       "  '27.',\n",
       "  '24',\n",
       "  '26.',\n",
       "  '34',\n",
       "  '27.',\n",
       "  '28',\n",
       "  '25.',\n",
       "  '26',\n",
       "  '26.',\n",
       "  '43',\n",
       "  '26.',\n",
       "  '32',\n",
       "  '24.',\n",
       "  '78',\n",
       "  '25.',\n",
       "  '54',\n",
       "  '22.',\n",
       "  '71',\n",
       "  'Generation',\n",
       "  'Down',\n",
       "  'am',\n",
       "  'B',\n",
       "  'Time',\n",
       "  'Oracle',\n",
       "  'Bo',\n",
       "  'Time',\n",
       "  'Best',\n",
       "  'Time',\n",
       "  'cate',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'ad',\n",
       "  '',\n",
       "  'pli',\n",
       "  'sider',\n",
       "  '-',\n",
       "  'early',\n",
       "  '',\n",
       "  '2',\n",
       "  'q',\n",
       "  '',\n",
       "  '',\n",
       "  '144',\n",
       "  '',\n",
       "  '83',\n",
       "  '+',\n",
       "  'B',\n",
       "  '(3',\n",
       "  '',\n",
       "  '96',\n",
       "  '×',\n",
       "  '10',\n",
       "  '−',\n",
       "  '5',\n",
       "  '',\n",
       "  '+3',\n",
       "  '',\n",
       "  '.',\n",
       "  '33',\n",
       "  '×',\n",
       "  '10',\n",
       "  '−',\n",
       "  '5',\n",
       "  'N',\n",
       "  '2',\n",
       "  ')',\n",
       "  'N',\n",
       "  '',\n",
       "  '7',\n",
       "  '#',\n",
       "  'ences',\n",
       "  '1000',\n",
       "  '50',\n",
       "  'podcast',\n",
       "  'GPU',\n",
       "  '3',\n",
       "  'Gi',\n",
       "  '',\n",
       "  '=1',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  'se',\n",
       "  'ization',\n",
       "  'L',\n",
       "  '2',\n",
       "  '−',\n",
       "  'M',\n",
       "  'm',\n",
       "  '',\n",
       "  '=1',\n",
       "  'log',\n",
       "  'P',\n",
       "  'y',\n",
       "  'm',\n",
       "  '',\n",
       "  '|',\n",
       "  '<',\n",
       "  'm',\n",
       "  '',\n",
       "  'X',\n",
       "  '(4)',\n",
       "  '',\n",
       "  '',\n",
       "  '-2',\n",
       "  'N',\n",
       "  '1',\n",
       "  '',\n",
       "  '(',\n",
       "  'z',\n",
       "  'i',\n",
       "  'log',\n",
       "  '',\n",
       "  'ˆ',\n",
       "  '(1',\n",
       "  '−',\n",
       "  ')',\n",
       "  'log',\n",
       "  '(1',\n",
       "  '−',\n",
       "  '',\n",
       "  'ˆ',\n",
       "  '',\n",
       "  'W',\n",
       "  'T',\n",
       "  's',\n",
       "  'h',\n",
       "  'i',\n",
       "  '-',\n",
       "  'socia',\n",
       "  'W',\n",
       "  '',\n",
       "  'loss',\n",
       "  'γ',\n",
       "  'label',\n",
       "  '(1',\n",
       "  '−',\n",
       "  'γ',\n",
       "  '2',\n",
       "  '(7)',\n",
       "  'ling',\n",
       "  '2',\n",
       "  'se',\n",
       "  '8',\n",
       "  'i',\n",
       "  '',\n",
       "  ',',\n",
       "  'label',\n",
       "  ')',\n",
       "  '',\n",
       "  'ˆ',\n",
       "  'z',\n",
       "  'i',\n",
       "  '',\n",
       "  ',',\n",
       "  'se',\n",
       "  '2',\n",
       "  'se',\n",
       "  'M',\n",
       "  'm',\n",
       "  '',\n",
       "  '=1',\n",
       "  'α',\n",
       "  's',\n",
       "  'm',\n",
       "  ',',\n",
       "  '(8)',\n",
       "  '7',\n",
       "  's',\n",
       "  '8',\n",
       "  'search',\n",
       "  '',\n",
       "  '4',\n",
       "  'α',\n",
       "  'm',\n",
       "  ',',\n",
       "  'der',\n",
       "  'step',\n",
       "  'm',\n",
       "  'over',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'φ',\n",
       "  '',\n",
       "  '|',\n",
       "  'X',\n",
       "  '',\n",
       "  'ns',\n",
       "  'label',\n",
       "  '',\n",
       "  'ns',\n",
       "  '2',\n",
       "  'se',\n",
       "  '(9)',\n",
       "  '',\n",
       "  '',\n",
       "  'la',\n",
       "  '',\n",
       "  'riza',\n",
       "  '',\n",
       "  'System',\n",
       "  '%',\n",
       "  'Re',\n",
       "  'call',\n",
       "  'R',\n",
       "  '1',\n",
       "  'R',\n",
       "  '2',\n",
       "  '',\n",
       "  'RL',\n",
       "  'Att',\n",
       "  'n',\n",
       "  'L',\n",
       "  'se',\n",
       "  '2',\n",
       "  'se',\n",
       "  ')',\n",
       "  '38',\n",
       "  '.',\n",
       "  '85',\n",
       "  '26.',\n",
       "  '90',\n",
       "  '9.',\n",
       "  '70',\n",
       "  '18.',\n",
       "  '78',\n",
       "  'Ex',\n",
       "  't',\n",
       "  'L',\n",
       "  'label',\n",
       "  ')',\n",
       "  '35',\n",
       "  '.',\n",
       "  '26',\n",
       "  '26.',\n",
       "  '39',\n",
       "  '8.',\n",
       "  '90',\n",
       "  '18.',\n",
       "  '03',\n",
       "  'M',\n",
       "  'L',\n",
       "  'M',\n",
       "  ')',\n",
       "  '40',\n",
       "  '.',\n",
       "  '50',\n",
       "  '27.',\n",
       "  '28',\n",
       "  '9.',\n",
       "  '82',\n",
       "  '19.00',\n",
       "  \"'\"])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, garb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65e24f23-48e8-49ea-a1a5-b31217406814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  'text': '6.1 Spotify Podcast results In Table  5 , a performance gain is obtained in all settings by adding MCS. By comparing different configurations with MCS, it can be seen that the gain from MCS in LoBART(8k) system is the low- est. This is because the average length is 5,727, meaning that many Podcasts inputs to LoBART(8k) do not benefit from content selection. CUED-filt, the best single-model system in  Man- akul and Gales  ( 2020 ), uses an attention-based con- tent selection at both training and test time, and it is combined with fine-tuned vanilla BART. Our approach outperforms CUED-filt by improved con- tent selection at both training time and test time as demonstrated by BART(1k)-ORC+MCS. Addition- ally, local self-attention allows training on longer sequences, and our LoBART(4k)-ORC+MCS sys- tem has yielded the best results. Lastly, even though LoBART(8k) requires more resource to train, it does not perform as well as LoBART(4k) due to its smaller attention window, and it also has a lower improvement when adding MCS. System CS-trn CS-tst R1 R2 RL CUED-filt ∗ \\x13 \\x13 26.96 9.75 18.90 BART(1k) \\x17 \\x17 26.43 9.22 18.35 BART(1k) \\x17 MCS 26.82 9.39 18.57 BART(1k) ORC \\x17 25.54 9.00 17.83 BART(1k) ORC MCS 27.28 9.82 19.00 LoBART(4k) \\x17 \\x17 27.02 9.57 18.78 LoBART(4k) \\x17 MCS 27.53 9.95 19.08 LoBART(4k) ORC \\x17 27.36 10.04 19.33 LoBART(4k) ORC MCS 27.81 10.30 19.61 LoBART(8k) \\x17 \\x17 26.90 9.47 18.50 LoBART(8k) \\x17 MCS 27.02 9.52 18.62 LoBART(8k) ORC \\x17 27.16 9.84 19.08 LoBART(8k) ORC MCS 27.49 9.98 19.25 6.2 ArXiv and PubMed results To verify the effectiveness of our systems, we re-train BART(1k) and LoBART(4k) on arXiv and PubMed datasets. Our training is different from Ext+TLM ( Pilault et al. ,  2020 ) where their abstractive models are trained using inputs ex- tracted from top two sentences in ROUGE recall for each target sentence without padding, similar to ORC no-pad . Although in 1k setting, ORC no-pad yields %AgORC no-pad  (defined in Section  5.1 ) of only 2.8% on arXiv (12% on PubMed), in 4k set- ting this is 39% on arXiv (71% on PubMed). Based on the best configurations on podcast data, we train BART(1k) and LoBART(4k) using TRC or ORC pad-rand  content selection, and we train the hi- erarchical model on arXiv/PubMed for MCS. ArXiv. In Table  6 , both BART(1k)+MCS and LoBART(4k)+MCS outperform all existing sys- tems. To better understand the advantages of our approach, the following systems are compared: Type System arXiv PubMed R1 R2 RL R1 R2 RL Previous Work Abs Discourse-Aware ( Cohan et al. ,  2018 ) 35.80 11.05 31.80 38.93 15.37 35.21 Mix Ext+TLM ( Pilault et al. ,  2020 ) 41.62 14.69 38.03 42.13 16.27 39.21 Ext ExtSum-LG+Rd( Xiao and Carenini ,  2020 ) 44.01 17.79 39.09 45.30 20.42 40.95 Abs Pegasus ( Zhang et al. ,  2020 ) 44.21 16.95 38.83 45.97 20.15 41.34 Abs DANCER ( Gidiotis and Tsoumakas ,  2020 ) 45.01 17.60 40.56 46.34 19.97 42.42 Abs BigBird(3k) ( Zaheer et al. ,  2020 ) 46.63 19.02 41.77 46.32 20.65 42.33 Abs LED(4k) ( Beltagy et al. ,  2020 ) 44.40 17.94 39.76 - - - Abs LED(16k) ( Beltagy et al. ,  2020 ) 46.63 19.62 41.83 - - - Mix CTRLsum(BART+BERT) ( He et al. ,  2020 ) 46.91 18.02 42.14 - - - This Work Abs † BART(1k) 44.96 17.25 39.76 45.06 18.27 40.84 Mix ‡ BART(1k)+MCS 47.68 19.77 42.25 46.49 19.45 42.04 Abs ‡ LoBART(4k) 46.59 18.72 41.24 47.47 20.47 43.02 Mix ‡ LoBART(4k)+MCS 48.79 20.55 43.31 48.06 20.96 43.56 CTRLsum versus our BART(1k) baseline; LED and BigBird versus our LoBART(4k) system. CTRLsum extends BART by conditioning it with extracted keywords  v  using a BERT-based model, e.g.  p ( y | X ,  v ) . Their BERT-based model uses sliding window allowing it to extract  v  in long sequences, but their BART is still limited to the first 1,024 tokens. As a result, it performs better than BART(1k), but worse than BART(1k)+MCS. LoBART(4k) has a similar architecture to LED(4k) without the global attention pattern for special tokens. Instead, our LoBART(4k) benefits from knowledge transferred from CNNDM and the ORC pad-rand  training-time content selection, which yields a larger gain when MCS is applied, i.e. the system trained with truncated data has a smaller gain when MCS is applied. Transfer learning com- parison and additional results on the impact of ORC pad-rand  are provided in Appendix  C . Compared to BigBird, LoBART(4k) has a longer input span, e.g. 3,072 vs. 4,096. However, BigBird benefits from utilizing more recent summarization specific pre-training Pegasus ( Zhang et al. ,  2020 ) which is better than our transfer learning. BigBird incorporates a global attention pattern similar to LED, and it also has a random attention pattern. Hence, LoBART without MCS performs worse. Ultimately, we show that adding MCS to either BART(1k) or LoBART(4k) yields a significant im- provement, resulting in state-of-the-art results in both settings. Moreover, although the gain from adding MCS is comparable to the gain observed in extending LED(4k) to LED(16k), the content selection method adds less training cost. PubMed.  Similarly, LoBART(4k)+MCS achieves state-of-the-art results shown in Table  6 . In con- trast to the arXiv results, BART(1k)+MCS does not outperform LoBART(4k) nor BigBird, and the gain from MCS is not as high in both 1k and 4k settings. 6.3 Local Attention v.s. MCS. Local attention yields better performance on PubMed, while MCS yields better performance on arXiv. To understand this discrepancy, a fine- grained analysis is conducted. 0 2000 4000 6000 8000 10000 12000 14000 16000 Average input length in each partition -1.0 0.0 1.0 2.0 3.0 4.0 5.0 Improvement in ROUGE-1 BART(1k) BART(1k)+MCS LoBART(4k) LoBART(4k)+MCS (a) arXiv (Len:Avg=8,584, 90 th %=16,108) 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Average input length in each partition 0.0 1.0 2.0 3.0 4.0 Improvement in ROUGE-1 BART(1k) BART(1k)+MCS LoBART(4k) LoBART(4k)+MCS (b) PubMed (Len:Avg=3,865, 90 th %=7,234) In Figure  6 , we partition the test sets by input lengths, and we evaluate the performance improve- ment in each partition with respect to the BART(1k) baseline. 9   The results illustrate that as the input length  N  increases: •  The improvement of systems  with  MCS in- creases and subsequently plateaus out. •  The improvement of systems  without  MCS decreases once the input exceeds the length limit but then plateaus, suggesting that fixed- span systems without content selection per- form worse once the maximum fixed-span is reached. For instance, below 4,000 input words, LoBART(4k) without MCS performs better than BART(1k)+MCS on both datasets. Therefore, our MCS method is more effective on arXiv compared to PubMed because the average length of PubMed documents is more than twice shorter than the average length of arXiv documents. '},\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
