{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets)\n",
      "  Downloading pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, safetensors, pytz, xxhash, tzdata, tqdm, regex, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.0 dill-0.3.6 frozenlist-1.3.3 fsspec-2023.6.0 huggingface-hub-0.15.1 multidict-6.0.4 multiprocess-0.70.14 pandas-2.0.2 pyarrow-12.0.1 pytz-2023.3 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.2 tzdata-2023.3 xxhash-3.2.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943421b5f44b470fadfcd80d5a0248f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem(x):\n",
    "    s = x.split(\"--\")\n",
    "    if len(s) < 2:\n",
    "        return x\n",
    "    else:\n",
    "        return \"--\".join(s[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_num(max_num):\n",
    "    return torch.randint(max_num,(1,))[0]\n",
    "\n",
    "def random_place(max_idx, num):\n",
    "    rand = torch.randperm(max_idx)[:num].sort().values\n",
    "    return rand[5:] if len(rand) > 8 else rand\n",
    "\n",
    "def random_token():\n",
    "    rand = tokenizer.bos_token_id\n",
    "    while rand in tokenizer.all_special_ids:\n",
    "        rand = torch.randint(tokenizer.vocab_size,(1,))[0]\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bos_eos(list_seq):\n",
    "    return torch.cat([torch.tensor([tokenizer.bos_token_id]), torch.tensor(list_seq), torch.tensor([tokenizer.eos_token_id])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(input_text, max_seq, rto):\n",
    "    enc = tokenizer.encode(input_text)[1:-1]\n",
    "    ll = min(round(max_seq * (1-rto)), len(enc))\n",
    "    enc = enc[:ll]\n",
    "    rd = random_place(ll, random_num(min(max_seq, round(ll / (1-rto))) - ll))\n",
    "\n",
    "    collect = []\n",
    "    onehot = []\n",
    "\n",
    "    idx = 0\n",
    "    l = len(rd)\n",
    "    for i in range(ll):\n",
    "        if i < l and i == rd[idx]:\n",
    "            collect += [random_token()]\n",
    "            onehot += [1]\n",
    "            idx += 1\n",
    "        \n",
    "        collect += [enc[i]]\n",
    "        onehot += [0]\n",
    "\n",
    "    input_ids = torch.ones(max_seq, dtype=torch.int64) * tokenizer.pad_token_id\n",
    "    e_col = bos_eos(collect)\n",
    "    input_ids[:len(e_col)] = e_col\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    onehot = torch.tensor([0] + onehot + [0], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    label = torch.zeros((onehot.shape[0], max_seq))\n",
    "    label[:, :onehot.shape[1]] = onehot\n",
    "\n",
    "    return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "class RoBERTa_Denoiser(nn.Module):\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.roberta_encoder = AutoModel.from_pretrained(\"xlm-roberta-large\")\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden_state = self.roberta_encoder(input_ids, attention_mask).last_hidden_state\n",
    "\n",
    "        output = torch.zeros((hidden_state.shape[0], 512, 1024)).to(self.device)\n",
    "        output[:, :hidden_state.shape[1], :] = hidden_state\n",
    "\n",
    "        output = self.head(output).squeeze(-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "denoiser = RoBERTa_Denoiser(device).to(device)\n",
    "denoiser.load_state_dict(torch.load(\"denoiser_roberta_rto_10000.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(pred, labels, weight):\n",
    "    sigpred = pred.sigmoid()\n",
    "    return (-(labels * sigpred.log() + (1-labels) * (1-sigpred).log()) * (labels * (weight-1) + 1)).sum()\n",
    "\n",
    "optimizer = optim.Adagrad(denoiser.parameters(), lr=3e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataload(i, n, max_seq, noise_rto):\n",
    "    b_input_ids = []\n",
    "    b_labels = []\n",
    "    for d in cnn[\"train\"][\"article\"][i:i+n]:\n",
    "        input_ids, label = noise(rem(d), max_seq, noise_rto)\n",
    "        b_input_ids += [input_ids]\n",
    "        b_labels += [label]\n",
    "\n",
    "    return torch.cat(b_input_ids, dim=0), torch.cat(b_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "max_seq_len = 512\n",
    "print_size = 50\n",
    "weight = 25\n",
    "noise_rto = 0.9\n",
    "\n",
    "start_point = 8000\n",
    "num = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2886766052246095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/250 [01:15<05:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.30107318878174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 100/250 [02:29<03:44,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.65401039123535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 150/250 [03:45<02:33,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.8227783203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 200/250 [05:04<01:14,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.84605094909668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [06:20<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "buf = 0\n",
    "denoiser.train()\n",
    "for i in tqdm(range(start_point, start_point+num, batch_size)):\n",
    "    input_ids, labels = dataload(i, batch_size, max_seq_len, noise_rto)\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    pred = denoiser(input_ids)\n",
    "    loss = criterion(pred, labels, weight)\n",
    "    buf += loss.item()\n",
    "    if i % (batch_size * print_size) == 0:\n",
    "        print(buf / print_size)\n",
    "        buf = 0\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(denoiser.state_dict(), \"./denoiser_roberta_rto_10000.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1265 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, labels = dataload(113450, 1, max_seq_len, 0.3)\n",
    "labels.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = denoiser(input_ids.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  16],\n",
       "        [  0,  18],\n",
       "        [  0,  29],\n",
       "        [  0,  31],\n",
       "        [  0,  33],\n",
       "        [  0,  35],\n",
       "        [  0,  38],\n",
       "        [  0,  49],\n",
       "        [  0,  52],\n",
       "        [  0,  55],\n",
       "        [  0,  57],\n",
       "        [  0,  59],\n",
       "        [  0,  61],\n",
       "        [  0,  63],\n",
       "        [  0,  68],\n",
       "        [  0,  70],\n",
       "        [  0,  72],\n",
       "        [  0,  75],\n",
       "        [  0,  77],\n",
       "        [  0,  81],\n",
       "        [  0,  86],\n",
       "        [  0,  89],\n",
       "        [  0,  92],\n",
       "        [  0,  94],\n",
       "        [  0, 102],\n",
       "        [  0, 105],\n",
       "        [  0, 107],\n",
       "        [  0, 109],\n",
       "        [  0, 112],\n",
       "        [  0, 115],\n",
       "        [  0, 117],\n",
       "        [  0, 120],\n",
       "        [  0, 123],\n",
       "        [  0, 125],\n",
       "        [  0, 139],\n",
       "        [  0, 141],\n",
       "        [  0, 146],\n",
       "        [  0, 152],\n",
       "        [  0, 154],\n",
       "        [  0, 156],\n",
       "        [  0, 164],\n",
       "        [  0, 167],\n",
       "        [  0, 170]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argwhere(pred > 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  16],\n",
       "        [  0,  18],\n",
       "        [  0,  29],\n",
       "        [  0,  31],\n",
       "        [  0,  33],\n",
       "        [  0,  35],\n",
       "        [  0,  38],\n",
       "        [  0,  49],\n",
       "        [  0,  52],\n",
       "        [  0,  55],\n",
       "        [  0,  57],\n",
       "        [  0,  59],\n",
       "        [  0,  61],\n",
       "        [  0,  63],\n",
       "        [  0,  68],\n",
       "        [  0,  70],\n",
       "        [  0,  72],\n",
       "        [  0,  75],\n",
       "        [  0,  77],\n",
       "        [  0,  81],\n",
       "        [  0,  86],\n",
       "        [  0,  89],\n",
       "        [  0,  92],\n",
       "        [  0,  94],\n",
       "        [  0, 102],\n",
       "        [  0, 105],\n",
       "        [  0, 107],\n",
       "        [  0, 109],\n",
       "        [  0, 112],\n",
       "        [  0, 115],\n",
       "        [  0, 117],\n",
       "        [  0, 120],\n",
       "        [  0, 123],\n",
       "        [  0, 125],\n",
       "        [  0, 139],\n",
       "        [  0, 141],\n",
       "        [  0, 146],\n",
       "        [  0, 152],\n",
       "        [  0, 154],\n",
       "        [  0, 156],\n",
       "        [  0, 164],\n",
       "        [  0, 167],\n",
       "        [  0, 170]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argwhere(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bos_eos(list_seq):\n",
    "    return torch.cat([torch.tensor([tokenizer.bos_token_id]), torch.tensor(list_seq), torch.tensor([tokenizer.eos_token_id])], dim=0)\n",
    "\n",
    "def denoise(text, max_seq_len=512):\n",
    "    enc = tokenizer.encode(text)[1:-1]\n",
    "    ll = len(enc)\n",
    "    chunk = max_seq_len - 2\n",
    "\n",
    "    ret = []\n",
    "    garb = []\n",
    "    for i in range(0, ll, chunk):\n",
    "        input_ids = bos_eos(enc[i:i+chunk]).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            remove = (denoiser(input_ids) > 0).to(\"cpu\")\n",
    "        \n",
    "        for j, k in zip(input_ids[0,1:-1], remove[0,1:-1]):\n",
    "            if not k:\n",
    "                ret += [j]\n",
    "            else:\n",
    "                garb += [tokenizer.decode([j])]\n",
    "\n",
    "    return tokenizer.decode(ret), garb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"  'text': '6.1 Spotify Podcast results In Table  5 , a performance gain is obtained in all settings by adding MCS. By comparing different configurations with MCS, it can be seen that the gain from MCS in LoBART(8k) system is the low- est. This is because the average length is 5,727, meaning that many Podcasts inputs to LoBART(8k) do not benefit from content selection. CUED-filt, the best single-model system in  Man- akul and Gales  ( 2020 ), uses an attention-based con- tent selection at both training and test time, and it is combined with fine-tuned vanilla BART. Our approach outperforms CUED-filt by improved con- tent selection at both training time and test time as demonstrated by BART(1k)-ORC+MCS. Addition- ally, local self-attention allows training on longer sequences, and our LoBART(4k)-ORC+MCS sys- tem has yielded the best results. Lastly, even though LoBART(8k) requires more resource to train, it does not perform as well as LoBART(4k) due to its smaller attention window, and it also has a lower improvement when adding MCS. System CS-trn CS-tst R1 R2 RL CUED-filt ∗ \\x13 \\x13 26.96 9.75 18.90 BART(1k) \\x17 \\x17 26.43 9.22 18.35 BART(1k) \\x17 MCS 26.82 9.39 18.57 BART(1k) ORC \\x17 25.54 9.00 17.83 BART(1k) ORC MCS 27.28 9.82 19.00 LoBART(4k) \\x17 \\x17 27.02 9.57 18.78 LoBART(4k) \\x17 MCS 27.53 9.95 19.08 LoBART(4k) ORC \\x17 27.36 10.04 19.33 LoBART(4k) ORC MCS 27.81 10.30 19.61 LoBART(8k) \\x17 \\x17 26.90 9.47 18.50 LoBART(8k) \\x17 MCS 27.02 9.52 18.62 LoBART(8k) ORC \\x17 27.16 9.84 19.08 LoBART(8k) ORC MCS 27.49 9.98 19.25 6.2 ArXiv and PubMed results To verify the effectiveness of our systems, we re-train BART(1k) and LoBART(4k) on arXiv and PubMed datasets. Our training is different from Ext+TLM ( Pilault et al. ,  2020 ) where their abstractive models are trained using inputs ex- tracted from top two sentences in ROUGE recall for each target sentence without padding, similar to ORC no-pad . Although in 1k setting, ORC no-pad yields %AgORC no-pad  (defined in Section  5.1 ) of only 2.8% on arXiv (12% on PubMed), in 4k set- ting this is 39% on arXiv (71% on PubMed). Based on the best configurations on podcast data, we train BART(1k) and LoBART(4k) using TRC or ORC pad-rand  content selection, and we train the hi- erarchical model on arXiv/PubMed for MCS. ArXiv. In Table  6 , both BART(1k)+MCS and LoBART(4k)+MCS outperform all existing sys- tems. To better understand the advantages of our approach, the following systems are compared: Type System arXiv PubMed R1 R2 RL R1 R2 RL Previous Work Abs Discourse-Aware ( Cohan et al. ,  2018 ) 35.80 11.05 31.80 38.93 15.37 35.21 Mix Ext+TLM ( Pilault et al. ,  2020 ) 41.62 14.69 38.03 42.13 16.27 39.21 Ext ExtSum-LG+Rd( Xiao and Carenini ,  2020 ) 44.01 17.79 39.09 45.30 20.42 40.95 Abs Pegasus ( Zhang et al. ,  2020 ) 44.21 16.95 38.83 45.97 20.15 41.34 Abs DANCER ( Gidiotis and Tsoumakas ,  2020 ) 45.01 17.60 40.56 46.34 19.97 42.42 Abs BigBird(3k) ( Zaheer et al. ,  2020 ) 46.63 19.02 41.77 46.32 20.65 42.33 Abs LED(4k) ( Beltagy et al. ,  2020 ) 44.40 17.94 39.76 - - - Abs LED(16k) ( Beltagy et al. ,  2020 ) 46.63 19.62 41.83 - - - Mix CTRLsum(BART+BERT) ( He et al. ,  2020 ) 46.91 18.02 42.14 - - - This Work Abs † BART(1k) 44.96 17.25 39.76 45.06 18.27 40.84 Mix ‡ BART(1k)+MCS 47.68 19.77 42.25 46.49 19.45 42.04 Abs ‡ LoBART(4k) 46.59 18.72 41.24 47.47 20.47 43.02 Mix ‡ LoBART(4k)+MCS 48.79 20.55 43.31 48.06 20.96 43.56 CTRLsum versus our BART(1k) baseline; LED and BigBird versus our LoBART(4k) system. CTRLsum extends BART by conditioning it with extracted keywords  v  using a BERT-based model, e.g.  p ( y | X ,  v ) . Their BERT-based model uses sliding window allowing it to extract  v  in long sequences, but their BART is still limited to the first 1,024 tokens. As a result, it performs better than BART(1k), but worse than BART(1k)+MCS. LoBART(4k) has a similar architecture to LED(4k) without the global attention pattern for special tokens. Instead, our LoBART(4k) benefits from knowledge transferred from CNNDM and the ORC pad-rand  training-time content selection, which yields a larger gain when MCS is applied, i.e. the system trained with truncated data has a smaller gain when MCS is applied. Transfer learning com- parison and additional results on the impact of ORC pad-rand  are provided in Appendix  C . Compared to BigBird, LoBART(4k) has a longer input span, e.g. 3,072 vs. 4,096. However, BigBird benefits from utilizing more recent summarization specific pre-training Pegasus ( Zhang et al. ,  2020 ) which is better than our transfer learning. BigBird incorporates a global attention pattern similar to LED, and it also has a random attention pattern. Hence, LoBART without MCS performs worse. Ultimately, we show that adding MCS to either BART(1k) or LoBART(4k) yields a significant im- provement, resulting in state-of-the-art results in both settings. Moreover, although the gain from adding MCS is comparable to the gain observed in extending LED(4k) to LED(16k), the content selection method adds less training cost. PubMed.  Similarly, LoBART(4k)+MCS achieves state-of-the-art results shown in Table  6 . In con- trast to the arXiv results, BART(1k)+MCS does not outperform LoBART(4k) nor BigBird, and the gain from MCS is not as high in both 1k and 4k settings. 6.3 Local Attention v.s. MCS. Local attention yields better performance on PubMed, while MCS yields better performance on arXiv. To understand this discrepancy, a fine- grained analysis is conducted. 0 2000 4000 6000 8000 10000 12000 14000 16000 Average input length in each partition -1.0 0.0 1.0 2.0 3.0 4.0 5.0 Improvement in ROUGE-1 BART(1k) BART(1k)+MCS LoBART(4k) LoBART(4k)+MCS (a) arXiv (Len:Avg=8,584, 90 th %=16,108) 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Average input length in each partition 0.0 1.0 2.0 3.0 4.0 Improvement in ROUGE-1 BART(1k) BART(1k)+MCS LoBART(4k) LoBART(4k)+MCS (b) PubMed (Len:Avg=3,865, 90 th %=7,234) In Figure  6 , we partition the test sets by input lengths, and we evaluate the performance improve- ment in each partition with respect to the BART(1k) baseline. 9   The results illustrate that as the input length  N  increases: •  The improvement of systems  with  MCS in- creases and subsequently plateaus out. •  The improvement of systems  without  MCS decreases once the input exceeds the length limit but then plateaus, suggesting that fixed- span systems without content selection per- form worse once the maximum fixed-span is reached. For instance, below 4,000 input words, LoBART(4k) without MCS performs better than BART(1k)+MCS on both datasets. Therefore, our MCS method is more effective on arXiv compared to PubMed because the average length of PubMed documents is more than twice shorter than the average length of arXiv documents. '},\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer.decode([random_token() for _ in range(max_seq_len-2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, garb = denoise(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"'text': '6.1 Podcast results In Table 5, a performance gain is obtained in all settings by adding MCS. By comparing different configurations with MCS, it can be seen that the gain from MCS in LoBART(8k) system is the low- est. This is because the average length is 5,727, meaning that manys inputs to LoBART(8k) do not benefit from content selection. CUED-filt, the best single-model system in Man- a and Gales ( ), uses an attention-based con- tent selection at both training and test time, and it is combined with fine-tuned vanilla BART. Our approach outperforms CUED-filt by improved con- tent selection at both training time and test time as demonstrated by BART(1k)-ORC+MCS. Addition- ally, local self-attention allows training on longer sequences, and our LoBART(4k)-ORC+MCS sys- tem has yielded the best results. Lastly, even though LoBART(8k) requires more resource to train, it does not perform as well as LoBART(4k) due to its smaller attention window, and it also has a lower improvement when adding MCS. System CS-trn CS-tst R1 R2 RL CUED-filt ∗ 26.96 9.75 18.90 BART(1k) 26.43 9.22 18.35 BART(1k) MCS 26.82 9.39 18.57 BART(1k) ORC 25.54 9.00 17.83 BART(1k) ORC MCS 27.28 9.82 19.00 LoBART(4k) 27.02 9.57 18.78 LoBART(4k) MCS 27.53 9.95 19.08 LoBART(4k) ORC 27.36 10.04 19.33 LoBART(4k) ORC MCS 27.81 10.30 19.61 LoBART(8k) 26.90 9.47 18.50 LoBART(8k) MCS 27.02 9.52 18.62 LoBART(8k) ORC 27.16 9.84 19.08 LoBART(8k) ORC MCS 27.49 9.98 19.25 6.2 ArXiv and PubMed results To verify the effectiveness of our systems, we re-train BARTk) and LoBART(4k) on arXiv and PubMed datasets. Our training is different from Ext+TLM ( Pilault et al., 2020 ) where their abstractive models are trained using inputs ex-ed from top two sentences in ROUGE for each sentence without padding, similar to ORC no-pad. Although in 1k setting, ORC no-pad yields %ORC no-pad (defined in Section 5.1 ) of only 2.8% on arXiv (12% on PubMed), in 4k set- ting this is 39% on arXiv (71% on PubMed). Based on the best configurations on data, we train BART(1k) and LoBART(4k) using TRC or ORC pad-rand content selection, and we train the hi- erarchical model on arXiv/PubMed for MCS. ArXiv. In Table 6, both BART(1k)+MCS and LoBART(4k)+MCS outperform all existing sys- tems. To better understand the advantages of our approach, the following systems are compared: Type System arXiv PubMed R1 R2 RL R1 R2 RL Previous Work Abs Discourse-Aware ( Cohan et al., 2018 ) 35.80 11.05 31.80 38.93 15.37 35.21 Mix Ext+TLM ( Pilault et al., 2020 ) 41.62 14.69 38.03 42.13 16.27 39.21 Ext ExtSum-LG+Rd( Xiao and Carenini, 2020 ) 44.01 17.79 39.09 45.30 20.42 40.95 Abs Pegasus ( Zhang et al., 2020 ) 44.21 16.95 38.83 45.97 20.15 41.34 Abs DANCER ( Gidiotis and Tsoumakas, 2020 ) 45.01 17.60 40.56 46.34 19.97 42.42 Abs BigBird(3k) ( Zaheer et al., 2020 ) 46.63 19.02 41.77 46.32 20.65 42.33 Abs LED(4k) ( Beltagy et al., 2020 ) 44.40 17.94 39.76 - - - Abs LED(16k) ( Beltagy et al., 2020 ) 46.63 19.62 41.83 - - - Mix CTRLsum(BART+BERT) ( et al., 2020 ) 46.91 18.02 42.14 - - - This Abs BART(1k) 44.96 17.25 39.76 45.06 18.27 40.84 Mix ‡ BART(1k)+MCS 47.68 19.77 42.25 46.49 19.45 42.04 Abs ‡ LoBART(4k) 46.59 18.72 41.24 47.47 20.47 43.02 Mix ‡ LoBART(4k)+MCS 48.79 20.55 43.31 48.06 20.96 43.56 CTRLsum versus our BART(1k) baseline; LED and BigBird versus our LoBART(4k) system. CTRLsum extends BART by conditioning it with extracted keywords v using a BERT-based model, e.g. p ( y | X, v ). Their BERT-based model uses sliding window allowing it to extract v in long sequences, but their BART is still limited to the first 1,024 tokens. As a result, it performs better than BART(1k), but worse than BART(1k)+MCS. LoBART(4k) has a similar architecture to LED(4k) without the global attention pattern for special tokens. Instead, our LoBART(4k) benefits from knowledge transferred from CNNDM and the ORC pad-rand training-time content selection, which yields a larger gain when MCS is applied, i.e. the system trained with truncated data has a smaller gain when MCS is applied. Transfer learning com- parison and additional results on the impact of ORC pad-rand are provided in Appendix C. Compared to BigBird, LoBART(4k) has a longer input span, e.g. 3,072 vs. 4,096. However, BigBird benefits from utilizing more recent summarization pre-training ( Zhang et al., 2020 ) which is better than our transfer learning. BigBird incorporates a global attention pattern similar to LED, and it also has a random attention pattern. Hence, LoBART without MCS performs worse. Ultimately, we show that adding MCS to either BART(1k) or LoBART(4k) yields a significant im-ment, resulting in state-of-the-art results in both settings. Moreover, although the gain from adding MCS is comparable to the gain observed in extending LED(4k) to LED(16k), the content selection method adds less training cost. PubMed. Similarly, LoBART(4k)+MCS achieves state-of-the-art results shown in Table 6. In con-t to the arXiv results, BART(1k)+MCS does not outperform LoBART(4k) nor BigBird, and the gain from MCS is not as high in both 1k and 4k settings. 6.3 Local Attention v.s. MCS. Local attention yields better performance on PubMed, while MCS yields better performance on arXiv. To understand this discrepancy, a fine-ed analysis is conducted. 0 2000 4000 6000 8000 10000 12000 14000 16000 Average input length in each partition -1.0 0.0 1.0 2.0 3.0 4.0 5.0 Improvement in ROUGE-1 BART(1k) BART(1k)+MCS LoBART(4k) LoBART(4k)+MCS (a) arXiv (Len:Avg=8,584, 90 th %=16,108) 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Average input length in each partition 0.0 1.0 2.0 3.0 4.0 Improvement in ROUGE-1 BART(1k) BART(1k)+MCS LoBART(4k) LoBART(4k)+MCS (b) PubMed (Len:Avg=3,865, 90 th %=7,234) In Figure 6, we partition the test sets by input lengths, and we evaluate the performance improve- ment in each partition with respect to the BART(1k) baseline. 9 The results illustrate that as the input length increases: • The improvement of systems with MCS in- creases and subsequently plateaus out. • The improvement of systems without MCS decreases once the input exceeds the length limit but then plateaus, suggesting that fixed- span systems without content selection per- form worse once the maximum fixed-span is reached. For instance, below 4,000 input words, LoBART(4k) without MCS performs better than BART(1k)+MCS on both datasets. Therefore, our MCS method is more effective on arXiv compared to PubMed because the average length of PubMed documents is more than twice shorter than the average length of arXiv documents. '},\",\n",
       " ['Spotify',\n",
       "  'Podcast',\n",
       "  'kul',\n",
       "  '2020',\n",
       "  '(1',\n",
       "  'tract',\n",
       "  'recall',\n",
       "  'target',\n",
       "  'Ag',\n",
       "  'podcast',\n",
       "  'He',\n",
       "  'Work',\n",
       "  '†',\n",
       "  'specific',\n",
       "  'Pegasus',\n",
       "  'prove',\n",
       "  'tras',\n",
       "  'grain',\n",
       "  'N'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, garb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
