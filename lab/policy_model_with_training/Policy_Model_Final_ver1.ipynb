{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6QKf3fJKdqb"
      },
      "source": [
        "## [0] Import\n",
        "\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-e0YyXaKN3F"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers datasets --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hyohsz_j-Wl",
        "outputId": "4d9847e1-137c-4670-abb1-83018f41f927"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pmy71DZqKAV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8749dd8c-c2ee-4151-8202-0e1ff67a257c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This code is written at 2023-06-22 03:47:11.821317\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "# Model Save & Load\n",
        "import os\n",
        "# GPU Reset\n",
        "from numba import cuda\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch import functional as F\n",
        "# from torchsummary import summary\n",
        "\n",
        "# 모델, Tokenizer Load\n",
        "from transformers import AutoModel, AutoTokenizer, LEDForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "\n",
        "# 데이터셋 Load from Summarize_from_feedvback, Huggingface\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "print(\"This code is written at \" + str(datetime.datetime.now()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPqk_9qbZQu"
      },
      "source": [
        "#### GPU Reset & Setting device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/23_Conference')"
      ],
      "metadata": {
        "id": "mKZnPGOSLSil"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aOi7uqSBbZQu"
      },
      "outputs": [],
      "source": [
        "def GPU_reset():\n",
        "    device = cuda.get_current_device()\n",
        "    device.reset\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(device)\n",
        "\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp-ey7E7bZQv",
        "outputId": "4ac86a61-9924-4a1d-b9cb-ce862f2ce2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = GPU_reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wz0P9-XCbZQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d53b485-7976-44ed-cff8-49d95257f43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun 22 03:47:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    28W /  70W |    103MiB / 15360MiB |      2%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g9Ib92VbZQw"
      },
      "source": [
        "## [*] Hyper Parameter Setting\n",
        "\n",
        "-------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2RjEmFnTbZQx"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1\n",
        "EPOCH = 1\n",
        "LEARNING_RATE = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM_Q7hSFR9QJ"
      },
      "source": [
        "## [2] DataLoad\n",
        "- 데이터 Load\n",
        "- 데이터 전처리\n",
        "- 데이터 Loader\n",
        "\n",
        "---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPpvZNh_tCyF"
      },
      "source": [
        "### [2.1] Data Preprocessing\n",
        "\n",
        "- Human Feedback 데이터 Dictionary를 Dataframe으로 변환\n",
        "- 알맞은 문장 추출\n",
        "- DataLoader 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcjuBVUzbZQz"
      },
      "source": [
        "#### Text DataFrame 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2hjUG5L4bZQz"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class Data_Preprocessing\n",
        "    - HuggingFace의 Summarize from feedback 전용 데이터 전처리 Class\n",
        "    - Train 데이터와 Validation 데이터 출력\n",
        "'''\n",
        "\n",
        "class Data_Preprocessing:\n",
        "    def __init__(self):\n",
        "        # DownLoad Data from huggingFace\n",
        "        ## Text Summary 데이터\n",
        "        ## CNN, TL;DR, Daily Mail\n",
        "        self.data_feedback = load_dataset(\"openai/summarize_from_feedback\", 'axis')\n",
        "\n",
        "        # Split into Train and Validation dataset\n",
        "        # Convert to DataFrame\n",
        "        self.df_train = pd.DataFrame(self.data_feedback['validation'])\n",
        "        self.df_valid = pd.DataFrame(self.data_feedback['test'])\n",
        "\n",
        "\n",
        "    # Original Text + Summarized Text 데이터 Columm 추출\n",
        "    def Data_cleaning(self, df_train, df_valid):\n",
        "        df_train['original_text'] = [row['post'] for row in df_train['info']]\n",
        "        df_valid['original_text'] = [row['article'] for row in df_valid['info']]\n",
        "\n",
        "        df_train['sum_text'] = [row['text'] for row in df_train['summary']]\n",
        "        df_valid['sum_text'] = [row['text'] for row in df_valid['summary']]\n",
        "\n",
        "        df_all = pd.concat([df_train[['original_text', 'sum_text']], df_valid[['original_text', 'sum_text']]], ignore_index=True)\n",
        "\n",
        "        return df_all\n",
        "\n",
        "    # 최종 DataFrame 출력\n",
        "    def data_complete_form(self):\n",
        "        df_train = self.Data_cleaning(self.df_train, self.df_valid).iloc[:-300, :]\n",
        "        df_valid = df_train.iloc[-300: , :].reset_index(drop=True)\n",
        "\n",
        "        return df_train, df_valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6OanIkksbZQ0"
      },
      "outputs": [],
      "source": [
        "# df_train.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlHf_VkpBQVl"
      },
      "source": [
        "### [2.2] DataLoader\n",
        "- 원본 Text와 Summarize가 합쳐진 데이터 형식의 DataFrame을 DataLoader로 처리\n",
        "- 입력: DataFrame <br>\n",
        "- Feature : original_with_good_sum,   original_with_bad_sum  </br>\n",
        "- 내용: 원본 텍스트 + 긍정 Summary, 원본 텍스트 + 부정 Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qfZKxT9WbZQ1"
      },
      "outputs": [],
      "source": [
        "class Policy_Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df_textsum): #, transforms_=None, random_masking = False,  unaligned=True ):\n",
        "\n",
        "        self.original_text = df_textsum['original_text']\n",
        "        self.sum_text = df_textsum['sum_text']\n",
        "\n",
        "        print(f\"My_dataset __init__ received : {self.original_text.shape}, {self.sum_text.shape}\")\n",
        "        print(f\"Data Type : {type(self.original_text[0]), type(self.sum_text[0])}\")\n",
        "        # print(f\"Data example : {self.original_text[0], {self.sum_text[0]}}\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        original_text = self.original_text[index]\n",
        "        sum_text = self.sum_text[index]\n",
        "\n",
        "        return original_text, sum_text\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sum_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  [3] All In ONe\n",
        "- Data Load 부터 Training, Inferecnce까지 담은 Class"
      ],
      "metadata": {
        "id": "TqyrQW3fEK-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IwuowiogbZQ4"
      },
      "outputs": [],
      "source": [
        "class Policy(Data_Preprocessing, Policy_Dataset):\n",
        "    def __init__(self,\n",
        "                 _batch_size = BATCH_SIZE,\n",
        "                 _epoch= EPOCH,\n",
        "                 _lr = LEARNING_RATE,\n",
        "                 _model_name = \"allenai/led-large-16384-arxiv\",\n",
        "                 _device = device):\n",
        "\n",
        "        # Device\n",
        "        self.device = _device\n",
        "\n",
        "        # Data\n",
        "        ## WARNING :: ONLY FOR FINETUNING\n",
        "        self.train_loader, self.valid_loader = self.finetuning_data_load(_batch_size)\n",
        "\n",
        "        # Tokenizer\n",
        "        self.tokenizer = self.policy_tokenizer(model_name = _model_name)\n",
        "\n",
        "        # Model\n",
        "        self.policy_model = self.policy_model(model_name = _model_name)\n",
        "\n",
        "        # Training\n",
        "        self.epoch = _epoch\n",
        "        self.optimizer = optim.AdamW(self.policy_model.parameters(), lr=_lr)\n",
        "\n",
        "        print(f\"\\n===================== POLICY INIT COMPLETE =====================\\n\")\n",
        "\n",
        "\n",
        "    ''' ========================================= D A T A  L O A D E R ===================================================='''\n",
        "    # WARNING :: ONLY FOR FINETUNING\n",
        "    def finetuning_data_load(self, batch_size):\n",
        "      df_train, df_valid = Data_Preprocessing().data_complete_form()\n",
        "      train_loader = torch.utils.data.DataLoader(Policy_Dataset(df_train), batch_size=batch_size, shuffle=False, drop_last = False)\n",
        "      valid_loader = torch.utils.data.DataLoader(Policy_Dataset(df_valid), batch_size=batch_size, shuffle=False, drop_last = False)\n",
        "\n",
        "      return train_loader, valid_loader\n",
        "\n",
        "\n",
        "    ''' ========================================= T O K E N I Z E R ===================================================='''\n",
        "\n",
        "    def policy_tokenizer(self, model_name = \"allenai/led-large-16384-arxiv\"):\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      return tokenizer\n",
        "\n",
        "    ''' ================================== G L O B A L  A T T E N T I O N  M A S K ======================================'''\n",
        "\n",
        "    def generate_global_attention_mask(self, tokenizer, input_ids):\n",
        "      mask = torch.torch.zeros_like(input_ids)\n",
        "      mask[((input_ids == tokenizer.bos_token_id) | (input_ids == tokenizer.eos_token_id)).nonzero(as_tuple=True)] = 1\n",
        "\n",
        "      return mask\n",
        "\n",
        "\n",
        "    ''' ========================================= 모  델  정  의 ===================================================='''\n",
        "\n",
        "    def policy_model(self, model_name=\"allenai/led-large-16384-arxiv\"):\n",
        "      policy_model = LEDForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "      return policy_model\n",
        "\n",
        "    ''' ========================================= 모  델  저  장 ===================================================='''\n",
        "\n",
        "    def save_model_info(self, _model, _version=\"ver_1\"):\n",
        "        if not os.path.isdir(\"./policy_model\"):\n",
        "            os.makedirs(\"./policy_model\")\n",
        "        # 모델 정보 저장\n",
        "        _model = _model.cpu()\n",
        "        torch.save({'model_state_dict': _model.state_dict(),\n",
        "                    # 'optimizer_state_dict': _optimizer.state_dict(),\n",
        "                    # 'record_list' : {'train_loss': _train_loss, 'valid_loss': _valid_loss},\n",
        "                    }, f\"./policy_model/policy_model_{_version}.pth\")  #policy_model_ver_1\n",
        "\n",
        "        print(f\"******************* Model Saved : policy_model_{_version} *******************\")\n",
        "\n",
        "\n",
        "    ''' ========================================== T R A I N I N G  ====================================================='''\n",
        "    # 모델 Training\n",
        "    def train(self):\n",
        "\n",
        "        ## 초기화\n",
        "        train_loss_list = []\n",
        "        valid_loss_list = []\n",
        "\n",
        "        record_train_loss = []\n",
        "        record_valid_loss = []\n",
        "\n",
        "        # Optimizer & Loss function\n",
        "        optimizer = self.optimizer\n",
        "\n",
        "        # Data Loader\n",
        "        train_loader = self.train_loader\n",
        "\n",
        "        # 모델 정의\n",
        "        model = self.policy_model\n",
        "        tokenizer = self.tokenizer\n",
        "        generate_global_attention_mask = self.generate_global_attention_mask\n",
        "\n",
        "        # Hyper Parameter\n",
        "        epoch = self.epoch\n",
        "        device = self.device\n",
        "\n",
        "        model.train()\n",
        "        for i in range(epoch):\n",
        "            start_time = time.time()\n",
        "\n",
        "            for index, (original_text, sum_text) in enumerate(train_loader):\n",
        "\n",
        "                original_token = tokenizer.batch_encode_plus(original_text, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "                sum_token = tokenizer.batch_encode_plus(sum_text, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "                original_attention_mask = generate_global_attention_mask(tokenizer, original_token).to(device)\n",
        "\n",
        "\n",
        "                output = model(input_ids = original_token,\n",
        "                               global_attention_mask = original_attention_mask,\n",
        "                               labels = sum_token)\n",
        "\n",
        "                loss = output[0]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                end_time = time.time()\n",
        "                train_loss_list.append(loss.item())\n",
        "\n",
        "                # RESULT PRINT OUT\n",
        "                if (index+1)%200 == 0:\n",
        "                    train_loss_mean = sum(train_loss_list) / len(train_loss_list)\n",
        "                    valid_loss_mean = self.validation(model, self.valid_loader, stop = 30)\n",
        "\n",
        "                    print(\"========================================================================================================================\")\n",
        "                    print(f\"Batch {(index+1)}  ({((index+1)/len(train_loader))*100 :.3f} %) \\t \\\n",
        "                            Train Loss : {train_loss_mean :.4f} \\t \\\n",
        "                            Valid Loss : {valid_loss_mean :.4f} \\t \\\n",
        "                            Elapsed Time: {(end_time - start_time) :.2f} sec\")\n",
        "\n",
        "                    train_loss_list = []\n",
        "                    # record_train_loss.append(train_loss_mean)\n",
        "                    # record_valid_loss.append(valid_loss_mean)\n",
        "\n",
        "                if (index+1)%1000 == 0:\n",
        "                    self.save_model_info(model, f\"ver_{(index+1)//1000}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    ''' ========================================== V A L I D A T I O N ====================================================='''\n",
        "    def validation(self, model, valid_loader, stop):\n",
        "      tokenizer = self.tokenizer\n",
        "      generate_global_attention_mask = self.generate_global_attention_mask\n",
        "\n",
        "      valid_loss_list = []\n",
        "      stop = stop\n",
        "      device = self.device\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for valid_index, (valid_original_text, valid_sum_text) in enumerate(valid_loader):\n",
        "\n",
        "            original_token = tokenizer.batch_encode_plus(valid_original_text, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "            sum_token = tokenizer.batch_encode_plus(valid_sum_text, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "            original_attention_mask = generate_global_attention_mask(tokenizer, original_token).to(device)\n",
        "\n",
        "            valid_output = model(input_ids = original_token,\n",
        "                                global_attention_mask = original_attention_mask,\n",
        "                                labels = sum_token)\n",
        "\n",
        "            valid_loss = valid_output[0]\n",
        "            valid_loss_list.append(valid_loss.item())\n",
        "\n",
        "            if (valid_index+1)%stop == 0:\n",
        "                break;\n",
        "\n",
        "      valid_loss_mean = sum(valid_loss_list) / len(valid_loss_list)\n",
        "      model.train()\n",
        "\n",
        "      return valid_loss_mean\n",
        "\n",
        "\n",
        "    ''' ========================================== L O A D  M O D E L ====================================================='''\n",
        "    def load_model_info(self, version):\n",
        "\n",
        "      file_path = f\"./policy_model/policy_model_{version}.pth\"\n",
        "\n",
        "      if not os.path.exists(file_path):\n",
        "          print(\"FATAL ERROR : model path not exist\")\n",
        "\n",
        "      model_info = torch.load(file_path)\n",
        "      print(f\"model_loaded : policy_model_{version}\")\n",
        "\n",
        "      model = self.policy_model\n",
        "      model.load_state_dict(model_info['model_state_dict'])\n",
        "      model.eval()\n",
        "\n",
        "      return model\n",
        "\n",
        "    ''' ========================================== I N F E R E N C E  ====================================================='''\n",
        "    def policy_inference(self, text, model):\n",
        "      device = self.device\n",
        "      # model = self.policy_model.to(device)\n",
        "      # model = self.load_model_info(version)\n",
        "      tokenizer = self.tokenizer\n",
        "\n",
        "      text_token = tokenizer.batch_encode_plus(text, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "      # text_global_att = self.generate_global_attention_mask(tokenizer, text_token).to(device)\n",
        "\n",
        "      summary_token = model.generate(inputs= text_token)\n",
        "      summary_token = summary_token\n",
        "\n",
        "      summarized_text = tokenizer.batch_decode(summary_token, skip_special_tokens=True)\n",
        "\n",
        "      return summarized_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize\n",
        "----"
      ],
      "metadata": {
        "id": "AYqH7j_iE3lv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "khE8LEelbZQ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "50799e0dea4f409ebf7e4df0b22d9876",
            "6c0dcfdc0ae946beb6d838920b7c75b9",
            "a0f953fd2f20431ebd3a41397e594e46",
            "bf875bb6885b4cf38dcf6b093fbf1cf0",
            "68b54f8b4dbe4777876b99928c8b2cdd",
            "8372bc24b59f49f097ba03bd3c50da63",
            "da160cabe07b402bb2c7cb93184248bd",
            "8e584c4daa4d4a908bd3f26130619a0f",
            "7289f203e8324171af181f071abd670f",
            "fc94e165fb1142909c1e6683a4bf0def",
            "f3412d89de3348f8a489756f30b5f12c"
          ]
        },
        "outputId": "321b0852-af35-4cce-ea90-73dbe10b0410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset summarize_from_feedback (/root/.cache/huggingface/datasets/openai___summarize_from_feedback/axis/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50799e0dea4f409ebf7e4df0b22d9876"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My_dataset __init__ received : (14597,), (14597,)\n",
            "Data Type : (<class 'str'>, <class 'str'>)\n",
            "My_dataset __init__ received : (300,), (300,)\n",
            "Data Type : (<class 'str'>, <class 'str'>)\n",
            "\n",
            "===================== POLICY INIT COMPLETE =====================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "policy = Policy(_batch_size = BATCH_SIZE,\n",
        "                _epoch= EPOCH,\n",
        "                _lr = LEARNING_RATE,\n",
        "                _model_name = \"allenai/led-large-16384-arxiv\",\n",
        "                _device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "-----"
      ],
      "metadata": {
        "id": "lcQKZREaE5ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = policy.train()"
      ],
      "metadata": {
        "id": "pvr4eVmjKQtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "----"
      ],
      "metadata": {
        "id": "1W2dmsM4GMop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"Ok so a bit of back story, my fiancee have been together 6 years. We have one 3 year old daughter together. We have had serious problems the last year. I found out she cheated on me with a coworker (March 2015). I've never been unfaithful to her, but I'm not perfect by any means. I don't believe I was being a good partner to her.. Not that it's any excuse to cheat. \\n\\nThe problem is we never went to counseling or anything, never really talked about it other than maybe that first week after I found out about it. She has a lot of depression and anxiety issues. We Co parent great, our sex life is good, we don't argue really. She just shuts down sometimes and gives up so to speak.\\n\\n Two days ago she tells me she just can't do it anymore. She feels hopeless etc. She is a stay at home mom now and money is tight for us with one income which has also caused issues. She says she loves me with all her heart but isn't in love like she was. \\n\\nAnd I know this all sounds like she's cheating again but I honestly don't think so. Should I let her leave, try to get to counseling? Just don't know what to do. Sorry for the long rambling post.\""
      ],
      "metadata": {
        "id": "Tl-7IipkHztk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (text,) # 모델안에서 tensor로 바꿔 처리하기 위해 Text를 tuple이나 리스트에 담아야 해요 ㅠㅠ"
      ],
      "metadata": {
        "id": "zP8OSXGSH5ld"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIWjvn3MLpXy",
        "outputId": "913ccaf7-80d0-432a-8221-bbb8ff13c690"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Ok so a bit of back story, my fiancee have been together 6 years. We have one 3 year old daughter together. We have had serious problems the last year. I found out she cheated on me with a coworker (March 2015). I've never been unfaithful to her, but I'm not perfect by any means. I don't believe I was being a good partner to her.. Not that it's any excuse to cheat. \\n\\nThe problem is we never went to counseling or anything, never really talked about it other than maybe that first week after I found out about it. She has a lot of depression and anxiety issues. We Co parent great, our sex life is good, we don't argue really. She just shuts down sometimes and gives up so to speak.\\n\\n Two days ago she tells me she just can't do it anymore. She feels hopeless etc. She is a stay at home mom now and money is tight for us with one income which has also caused issues. She says she loves me with all her heart but isn't in love like she was. \\n\\nAnd I know this all sounds like she's cheating again but I honestly don't think so. Should I let her leave, try to get to counseling? Just don't know what to do. Sorry for the long rambling post.\",)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# policy_model = policy.load_model_info(\"ver_2\")\n",
        "policy_model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\").to(device)"
      ],
      "metadata": {
        "id": "Nqbz73AYNvUk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = policy.policy_inference(text, model = policy_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuVfmfteCN5t",
        "outputId": "c94e1373-9889-405c-cd1c-d94c6f2cc0e9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avsyH2yICxbk",
        "outputId": "de8099d1-02fb-4d63-fb02-6b2a18b161d0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" my fiancee and i have been together 6 years. we have one 3 year old daughter together. we have had serious problems the last year.  two days ago she tells me she just can't do it anymore.  she says she loves me with all her heart but isn't in love like she was.  and she just shuts down sometimes and gives up so to speak.  the problem is we never went to counseling or anything, never really talked about it other than maybe that first week after I found out about it.  just don't know what to do. Should i let her leave, try to get to counseling? \"]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model\n",
        "----"
      ],
      "metadata": {
        "id": "ezjVMY6JNCea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_model = policy.load_model_info(\"ver_1\").to(device)"
      ],
      "metadata": {
        "id": "p0UlmPPZNEkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model\n",
        "----"
      ],
      "metadata": {
        "id": "xz-U_M6nGUoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy.save_model_info(policy_model, \"ver_x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rHo3fAmIyuF",
        "outputId": "4d987fc1-3741-4edf-ac2a-c99cacf0360a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************* Model Saved : policy_model_ver_x *******************\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Full on Python 3.7 (GPU)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50799e0dea4f409ebf7e4df0b22d9876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c0dcfdc0ae946beb6d838920b7c75b9",
              "IPY_MODEL_a0f953fd2f20431ebd3a41397e594e46",
              "IPY_MODEL_bf875bb6885b4cf38dcf6b093fbf1cf0"
            ],
            "layout": "IPY_MODEL_68b54f8b4dbe4777876b99928c8b2cdd"
          }
        },
        "6c0dcfdc0ae946beb6d838920b7c75b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8372bc24b59f49f097ba03bd3c50da63",
            "placeholder": "​",
            "style": "IPY_MODEL_da160cabe07b402bb2c7cb93184248bd",
            "value": "100%"
          }
        },
        "a0f953fd2f20431ebd3a41397e594e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e584c4daa4d4a908bd3f26130619a0f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7289f203e8324171af181f071abd670f",
            "value": 2
          }
        },
        "bf875bb6885b4cf38dcf6b093fbf1cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc94e165fb1142909c1e6683a4bf0def",
            "placeholder": "​",
            "style": "IPY_MODEL_f3412d89de3348f8a489756f30b5f12c",
            "value": " 2/2 [00:00&lt;00:00, 12.12it/s]"
          }
        },
        "68b54f8b4dbe4777876b99928c8b2cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8372bc24b59f49f097ba03bd3c50da63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da160cabe07b402bb2c7cb93184248bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e584c4daa4d4a908bd3f26130619a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7289f203e8324171af181f071abd670f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc94e165fb1142909c1e6683a4bf0def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3412d89de3348f8a489756f30b5f12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}